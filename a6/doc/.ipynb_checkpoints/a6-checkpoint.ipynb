{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 340 Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.sparse import csr_matrix as sparse_matrix\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={mechanics:5}\n",
    "\n",
    "\n",
    "The above points are allocated for following the [homework submission instructions](https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/home/blob/master/homework_instructions.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Finding similar items\n",
    "\n",
    "For this question we'll be using the [Amazon product data set](http://jmcauley.ucsd.edu/data/amazon/). The author of the data set has asked for the following citations:\n",
    "\n",
    "> Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering.\n",
    "> R. He, J. McAuley.\n",
    "> WWW, 2016.\n",
    "> \n",
    "> Image-based recommendations on styles and substitutes.\n",
    "> J. McAuley, C. Targett, J. Shi, A. van den Hengel.\n",
    "> SIGIR, 2015.\n",
    "\n",
    "We will focus on the \"Patio, Lawn, and Garden\" section. Download the [ratings](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Patio_Lawn_and_Garden.csv) and place the file in the `data` directory with the original filename. Once you do that, the code below should load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ratings_Patio_Lawn_and_Garden.csv\"\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", filename), \"rb\") as f:\n",
    "    ratings = pd.read_csv(f,names=(\"user\",\"item\",\"rating\",\"timestamp\"))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to construct the user-product matrix `X`. Let's see how big it would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ratings, item_key=\"item\", user_key=\"user\"):\n",
    "    print(\"Number of ratings:\", len(ratings))\n",
    "    print(\"The average rating:\", np.mean(ratings[\"rating\"]))\n",
    "\n",
    "    d = len(set(ratings[item_key]))\n",
    "    n = len(set(ratings[user_key]))\n",
    "    print(\"Number of users:\", n)\n",
    "    print(\"Number of items:\", d)\n",
    "    print(\"Fraction nonzero:\", len(ratings)/(n*d))\n",
    "    print(\"Size of full X matrix: %.2f GB\" % ((n*d)*8/1e9))\n",
    "\n",
    "    return n,d\n",
    "\n",
    "n,d = get_stats(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "600 GB! That is way too big. We don't want to create that matrix. On the other hand, we see that we only have about 1 million ratings, which would be around 8 MB ($10^6$ numbers $\\times$ at 8 bytes per double precision floating point number). Much more manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_X(ratings,n,d,user_key=\"user\",item_key=\"item\"):\n",
    "    user_mapper = dict(zip(np.unique(ratings[user_key]), list(range(n))))\n",
    "    item_mapper = dict(zip(np.unique(ratings[item_key]), list(range(d))))\n",
    "\n",
    "    user_inverse_mapper = dict(zip(list(range(n)), np.unique(ratings[user_key])))\n",
    "    item_inverse_mapper = dict(zip(list(range(d)), np.unique(ratings[item_key])))\n",
    "\n",
    "    user_ind = [user_mapper[i] for i in ratings[user_key]]\n",
    "    item_ind = [item_mapper[i] for i in ratings[item_key]]\n",
    "\n",
    "    X = sparse_matrix((ratings[\"rating\"], (user_ind, item_ind)), shape=(n,d))\n",
    "    \n",
    "    return X, user_mapper, item_mapper, user_inverse_mapper, item_inverse_mapper, user_ind, item_ind\n",
    "\n",
    "X, user_mapper, item_mapper, user_inverse_mapper, item_inverse_mapper, user_ind, item_ind = create_X(ratings, n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(X.shape) # should be number of users by number of items\n",
    "print(X.nnz)   # number of nonzero elements -- should equal number of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.data.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Above: verifying our estimate of 8 MB to store sparse `X`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Find the following items:\n",
    "\n",
    "1. the item with the most reviews\n",
    "2. the item with the most total stars\n",
    "3. the item with the highest average stars\n",
    "\n",
    "Then, find the names of these items by looking them up with the url https://www.amazon.com/dp/ITEM_ID, where `ITEM_ID` is the id of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_amazon = \"https://www.amazon.com/dp/%s\"\n",
    "\n",
    "# example:\n",
    "print(url_amazon % 'B00CFM0P7Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 1\n",
    "unique, count = np.unique(item_ind, return_counts=True)\n",
    "print(url_amazon % item_inverse_mapper[unique[np.argmax(count)]])\n",
    "print(\"Classic Accessories 73942 Veranda Grill Cover - Durable BBQ Cover with Heavy-Duty Weather Resistant Fabric, X-Large, 70-Inch\")\n",
    "print(\"\\n\");\n",
    "\n",
    "## 2\n",
    "df = pd.DataFrame(ratings)\n",
    "df = df.drop([\"user\", \"timestamp\"], axis=1)\n",
    "df = df.groupby(by=\"item\").sum()\n",
    "print(url_amazon % item_inverse_mapper[np.argmax(df.values[:,0])])\n",
    "print(\"Classic Accessories 73942 Veranda Grill Cover - Durable BBQ Cover with Heavy-Duty Weather Resistant Fabric, X-Large, 70-Inch\")\n",
    "print(\"\\n\");\n",
    "\n",
    "##3\n",
    "df = pd.DataFrame(ratings)\n",
    "df = df.drop([\"user\", \"timestamp\"], axis=1)\n",
    "df = df.groupby(by=\"item\").mean()\n",
    "print(url_amazon % item_inverse_mapper[np.argmax(df.values[:,0])])\n",
    "print(\"Primal Grill with Steven Raichlen, Volume One\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the following histograms \n",
    "\n",
    "1. The number of ratings per user\n",
    "2. The number of ratings per item\n",
    "3. The ratings themselves\n",
    "\n",
    "For the first two, use\n",
    "```\n",
    "plt.yscale('log', nonposy='clip')\n",
    "``` \n",
    "to put the histograms on a log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1\n",
    "ratings_per_user = np.sum(X != 0, axis=1)\n",
    "plt.hist(ratings_per_user)\n",
    "plt.title(\"Number of Ratings Per User\")\n",
    "plt.xlabel(\"Number Of Ratings\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "## 2\n",
    "unique, count = np.unique(item_ind, return_counts=True)\n",
    "plt.hist(count)\n",
    "plt.title(\"Number of Ratings Per Item\")\n",
    "plt.xlabel(\"Number Of Ratings\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "## 3\n",
    "rows, cols = X.nonzero()\n",
    "ratings_themselves = X[rows, cols][0: 1000]\n",
    "ratings_themselves = ratings_themselves.transpose()\n",
    "plt.hist(ratings_themselves, bins=[1, 2, 3, 4, 5])\n",
    "plt.title(\"Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Use scikit-learn's [NearestNeighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) object (which uses Euclidean distance by default) to find the 5 items most similar to [Brass Grill Brush 18 Inch Heavy Duty and Extra Strong, Solid Oak Handle](https://www.amazon.com/dp/B00CFM0P7Y). \n",
    "\n",
    "The code block below grabs the column of `X` associated with the grill brush. The mappers take care of going back and forther between the IDs (like `B00CFM0P7Y`) and the indices of the sparse array (0,1,2,...).\n",
    "\n",
    "Note: keep in mind that `NearestNeighbors` is for taking neighbors across rows, but here we're working across columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grill_brush = \"B00CFM0P7Y\"\n",
    "grill_brush_ind = item_mapper[grill_brush]\n",
    "grill_brush_vec = X[:,grill_brush_ind]\n",
    "\n",
    "print(url_amazon % grill_brush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=6)\n",
    "neigh.fit(X.transpose()) \n",
    "distances, indices = neigh.kneighbors(grill_brush_vec.transpose())\n",
    "print(indices[0,1:6])\n",
    "for i in range (1,6):\n",
    "    print(url_amazon % item_inverse_mapper[indices[0,i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Using cosine similarity instead of Euclidean distance in `NearestNeighbors`, find the 5 products most similar to `B00CFM0P7Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=6, metric='cosine')\n",
    "neigh.fit(X.transpose()) \n",
    "distances, indices = neigh.kneighbors(grill_brush_vec.transpose())\n",
    "print(indices[0,1:6])\n",
    "for i in range (1,6):\n",
    "    print(url_amazon % item_inverse_mapper[indices[0,i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For each of the two metrics, compute the compute the total popularity (total stars) of each of the 5 items and report it. Do the results make sense given what we discussed in class about Euclidean distance vs. cosine similarity? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = [103866, 103865, 98897, 72226, 102810]\n",
    "cosine = [103866, 103867, 103865, 98068, 98066]\n",
    "\n",
    "print(\"Grill brush total stars: {0}\\n\".format(grill_brush_vec.sum()))\n",
    "\n",
    "print(\"Euclidean distance:\")\n",
    "for i in range(5):\n",
    "    print(\"{0}: {1}\".format(euclidean[i], X[:,euclidean[i]].sum()))\n",
    "    \n",
    "print(\"\\nCosine similarity:\")\n",
    "for i in range(5):\n",
    "    print(\"{0}: {1}\".format(cosine[i], X[:,cosine[i]].sum()))\n",
    "    \n",
    "## In class we discussed how cosine similarity will find more popular items than euclidean distance.\n",
    "## This is shown by the results, the neighbours found while using cosine similarity have a much greater \n",
    "## total popularity than those found while using euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6\n",
    "rubric={reasoning:3}\n",
    "\n",
    "PCA gives us an approximation $X \\approx ZW$ where the rows of $Z$ contain a length-$k$ latent feature vectors for each user and the columns of $W$ contain a length-$k$ latent feature vectors for each item.\n",
    "\n",
    "Another strategy for finding similar items is to run PCA and then search for nearest neighbours with Euclidean distance in the latent feature space, which is hopefully more meaningful than the original \"user rating space\". In other words, we run nearest neighbors on the columns of $W$. Using $k=10$ and scikit-learn's [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) to perform the dimensionality reduction, find the 5 nearest neighbours to the grill brush using this method. You can access $W$ via the `components_` field of the `TruncatedSVD` object, after you fit it to the data. \n",
    "\n",
    "Briefly comment on your results.\n",
    "\n",
    "Implementation note: when you call on `NearestNeighbors.kneighbors`, it expects the input to be a 2D array. There's some weirdness here because `X` is a scipy sparse matrix but your `W` will be a dense matrix, and they behave differently in subtle ways. If you get an error like \"Expected 2D array, got 1D array instead\" then this is your problem: a column of `W` is technically a 1D array but a column of `X` has dimension $1\\times n$, which is technically a 2D array. You can take a 1D numpy array and add an extra first dimension to it with `array[None]`.\n",
    "\n",
    "Conceptual note 1: We are using the \"truncated\" rather than full SVD since a full SVD would involve dense $d\\times d$ matrices, which we've already established are too big to deal with. And then we'd only use the first $k$ rows of it anyway. So a full SVD would be both impossible and pointless.\n",
    "\n",
    "Conceptual note 2: as discussed in class, there is a problem here, which is that we're not ignoring the missing entries. You could get around this by optimizing the PCA objective with gradient descent, say using `findMin` from previous assignments. But we're just going to ignore that for now, as the assignment seems long enough as it is (or at least it's hard for me to judge how long it will take because it's new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=10)\n",
    "svd.fit(X)\n",
    "W = svd.components_\n",
    "grill_brush_vec = W[:,grill_brush_ind]\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=6)\n",
    "neigh.fit(W.transpose()) \n",
    "distances, indices = neigh.kneighbors(grill_brush_vec[None])\n",
    "print(indices[0,1:6])\n",
    "for i in range (1,6): \n",
    "    print(url_amazon % item_inverse_mapper[indices[0,i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: putting it all together in a CPSC 340 \"mini-project\"\n",
    "rubric={reasoning:25}\n",
    "\n",
    "In this open-ended mini-project, you'll explore the [UCI default of credit card clients data set](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). There are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default payment next month\" in the data. The rest of the columns can be used as features. \n",
    "\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Download the data set and load it in. Since the data comes as an MS Excel file, I suggest using [`pandas.read_excel`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html) to read it in. See [Lecture 2](https://github.ugrad.cs.ubc.ca/CPSC340-2017W-T2/home/blob/master/lectures/L2.ipynb) for an example of using pandas.\n",
    "2. Perform exploratory data analysis on the data set. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Randomly split the data into train, validation, test sets. The validation set will be used for your experiments. The test set should be saved until the end, to make sure you didn't overfit on the validation set. You are welcome to use scikit-learn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which takes care of both shuffling and splitting. \n",
    "4. Try scikit-learn's [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) as a baseline model.\n",
    "5. Try logistic regression as a first real attempt. Make a plot of train/validation error vs. regularization strength. What’s the lowest validation error you can get?\n",
    "6. Explore the features, which are described on the UCI site. Explore preprocessing the features, in terms of transforming non-numerical variables, feature scaling, change of basis, etc. Did this improve your results?\n",
    "7. Try 3 other models aside from logistic regression, at least one of which is a neural network. Can you beat logistic regression? (For the neural net(s), the simplest choice would probably be to use scikit-learn's [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), but you are welcome to use any software you wish. )\n",
    "8. Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. In at least one case you should be optimizing multiple hyperparameters for a single model. I won't make it a strict requirement, but I recommend checking out one of the following (the first two are simple scikit-learn tools, the latter two are much more sophisticated algorithms and require installing new packages): \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)\n",
    "9. Explore feature selection for this problem. What are some particularly relevant and irrelevant features? Can you improve on your original logistic regression model if you first remove some irrelevant features?\n",
    "10. Take your best model overall. Train it on the combined train/validation set and run it on the test set once. Does the test error agree fairly well with the validation error from before? Do you think you’ve had issues with optimization bias? Report your final test error directly in your README.md file as well as in your report.\n",
    "\n",
    "**Submission format:**\n",
    "Your submission should take the form of a \"report\" that includes both code and an explanation of what you've done. You don't have to include everything you ever tried - it's fine just to have your final code - but it should be reproducible. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code.\n",
    "\n",
    "**Assessment:**\n",
    "We plan to grade and fairly leniently. We don't have some secret target accuracy that you need to achieve to get a good grade. You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results. For example, if you write something like, \"And then I noticed the model was overfitting, so I decided to stop using regularization\" - then, well, that's not good. If you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "**And...**\n",
    "This style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (2-6 hours???) is a good guideline for a typical submission. Of course if you're having fun you're welcome to spend as much time as you want! But, if so, don't do it out of perfectionism... do it because you're learning and enjoying it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.sparse import csr_matrix as sparse_matrix\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# YOUR CODE AND REPORT HERE, IN A SENSIBLE FORMAT\n",
    "filename = \"default of credit card clients.xls\"\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", filename), \"rb\") as f:\n",
    "    credit = pd.read_excel(f,names=(\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\", \"default payment next month\"))\n",
    "credit.head()\n",
    "\n",
    "X = credit.values[1:,0:23]\n",
    "y = credit.values[1:,23:24]\n",
    "y=y.astype('int')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "# Split X and y into training set (75%) and test set(25%)\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(X, y)\n",
    "\n",
    "y_test = y_test.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.\n",
    "\n",
    "print(\"Likelihood a male will default on the next payment: %.3f\" % (y[X[:,1] == 1].sum() / len(y[X[:,1] == 1])))\n",
    "print(\"Likelihood a female will default on the next payment: %.3f\" % (y[X[:,1] == 2].sum() / len(y[X[:,1] == 2])))\n",
    "print(\"Likelihood of someone married to default: %.3f\" % (y[X[:,3] == 1].sum() / len(y[X[:,3] == 1])))\n",
    "\n",
    "## Histogram of the age of defaulters\n",
    "## Younger people are more likely to default than older people\n",
    "age = X[(y == 1)[:,0],4]\n",
    "age = age[None].transpose()\n",
    "plt.hist(age)\n",
    "plt.title(\"Age of Defaulters\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "## Histogram of the education level of defaulters\n",
    "## People with graduate or univeristy level education are more likely to default\n",
    "## education: 1 = graduate school; 2 = university; 3 = high school; 4 = others; 5,6 = undefined\n",
    "education = X[(y == 1)[:,0],2]\n",
    "education = education[None].transpose()\n",
    "plt.hist(education, bins=[1, 2, 3, 4, 5, 6])\n",
    "plt.title(\"Education of Defaulters\")\n",
    "plt.xlabel(\"Education\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 3.\n",
    "## Im using scikit learn's KFold method to divide the data into training and validation sets, using 10 fold cross\n",
    "## validation. \n",
    "\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.\n",
    "## Here we are fitting the dummy classifier\n",
    "i = 0\n",
    "dummy_tr_err = np.zeros(n_splits)\n",
    "dummy_va_err = np.zeros(n_splits)\n",
    "for train, valid in kf.split(X_rest):\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = X[train], X[valid], y[train], y[valid]\n",
    "    \n",
    "    # Ravel our y values\n",
    "    y_train = y_train.ravel()\n",
    "    y_valid = y_valid.ravel()\n",
    "\n",
    "    # Fit and test Dummy Classifier\n",
    "    clf = DummyClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)  \n",
    "    tr_error = np.mean(y_pred != y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_valid)  \n",
    "    va_error = np.mean(y_pred != y_valid)\n",
    "\n",
    "    dummy_tr_err[i] = tr_error\n",
    "    dummy_va_err[i] = va_error\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "## Output the mean of the training and validation error over our 10 folds\n",
    "print(\"Dummy Training error: %.3f\" % np.mean(dummy_tr_err))\n",
    "print(\"Dummy Validation error: %.3f\" % np.mean(dummy_va_err))\n",
    "\n",
    "## Here we calculate the test error\n",
    "clf = DummyClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "print(\"Dummy test error: %.3f\" % te_error)\n",
    "\n",
    "\n",
    "## Here is an example of the results returned by the dummy classifier:\n",
    "## Dummy Training error: 0.350\n",
    "## Dummy Validation error: 0.353\n",
    "## Dummy test error: 0.361\n",
    "\n",
    "## They arent amazing results, which is to be expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5\n",
    "## Here we are fitting logistic regression using L1 regularization. I found that it gave better results\n",
    "## than L2 regularization. For each fold, we loop over 7 possible regularization strengths and store the \n",
    "## reults in a 10 x 7 matrix.\n",
    "i = 0\n",
    "logreg_tr_err = np.zeros((n_splits,7))\n",
    "logreg_va_err = np.zeros((n_splits,7))\n",
    "# These are the regularization strengths to check\n",
    "strengths = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for train, valid in kf.split(X_rest):\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = X[train], X[valid], y[train], y[valid]\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    y_valid = y_valid.ravel()\n",
    "    \n",
    "    index = 0\n",
    "    for j in strengths:\n",
    "        # In scikit learn's logistic regression model, C = the inverse of the regularization strength\n",
    "        logreg = LogisticRegression(penalty='l1', C=1/j)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_train)  \n",
    "        tr_error = np.mean(y_pred != y_train)\n",
    "\n",
    "        y_pred = logreg.predict(X_valid)  \n",
    "        va_error = np.mean(y_pred != y_valid)\n",
    "\n",
    "        logreg_tr_err[i,index] = tr_error    \n",
    "        logreg_va_err[i,index] = va_error\n",
    "        index = index + 1\n",
    "    i = i + 1\n",
    "\n",
    "## Plot the training and validation error, taking the mean across folds \n",
    "## The x axis is put on a log scale since the regularization strengths are a logarithmic range\n",
    "f, fig = plt.subplots(1)\n",
    "fig.plot(loop, np.mean(logreg_tr_err, axis=0), label='training error')\n",
    "fig.plot(loop, np.mean(logreg_va_err, axis=0), label='validation error')\n",
    "plt.legend()\n",
    "plt.title(\"Training/Validation error Vs Lambda\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Classification Error\")\n",
    "plt.xscale('log', nonposy='clip')\n",
    "plt.show()\n",
    "\n",
    "## We report the minimum mean of the training and validation error over our 10 folds\n",
    "print(\"Lowest training error: %.3f\" % np.min(np.mean(logreg_tr_err, axis=0)))\n",
    "print(\"Lowest validation error: %.3f\" % np.min(np.mean(logreg_va_err, axis=0)))\n",
    "\n",
    "## Here we output the regularization strength that corresponded to the lowest error\n",
    "## during training and validation.\n",
    "print(\"Best Lambda during training: {0}\".format(strengths[np.argmin(np.mean(logreg_tr_err, axis=0))]))\n",
    "print(\"Best Lambda during validation: {0}\".format(strengths[np.argmin(np.mean(logreg_va_err, axis=0))]))\n",
    "\n",
    "## Here we fit the model to the training data using the best lambda that was found during validation.\n",
    "## We then use this model to calculate the test error\n",
    "logreg = LogisticRegression(penalty='l1', C=1/strengths[np.argmin(np.mean(logreg_va_err, axis=0))])\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "print(\"Logistic regression test error: %.3f\" % te_error)\n",
    "\n",
    "## Here is an example of the results returned, minus the graph:\n",
    "## Lowest training error: 0.193\n",
    "## Lowest validation error: 0.194\n",
    "## Best Lambda during training: 0.01\n",
    "## Best Lambda during validation: 10\n",
    "## Logistic regression test error: 0.187\n",
    "\n",
    "## These results are much better than our dummy classifier, our training, validation, and test error are all\n",
    "## much lower. We tuned our regularization strength using cross validation and as a result, have quite a good\n",
    "## test error. Interestingly, our test error is lower than both our training and validation error for this \n",
    "## example run. This may be caused by the way our data was split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6\n",
    "## Since our dataset does not have that many features, feature selection doesn't help very much.\n",
    "## Scaling the bill up and the limit down (putting more weight on bill and less on limit) seems to\n",
    "## to decrease the error slightly\n",
    "\n",
    "X_new = X_rest\n",
    "y_rest = y_rest.ravel()\n",
    "\n",
    "logreg = LogisticRegression(penalty='l1', C = 1/10)\n",
    "logreg.fit(X_new, y_rest)\n",
    "y_pred = logreg.predict(X_new)  \n",
    "tr_error = np.mean(y_pred != y_rest)\n",
    "\n",
    "y_pred = logreg.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "    \n",
    "print(\"before scale training error %.3f\" % tr_error)\n",
    "print(\"before scale test error %.3f\" % te_error)\n",
    "\n",
    "## scale bill\n",
    "for i in range(len(y_rest)):\n",
    "    for j in range(11, 22):\n",
    "        X_new[i,j] = X_new[i,j] * 100\n",
    "    \n",
    "## scale limit\n",
    "for i in range(len(y_rest)):\n",
    "    X_new[i,0] = X_new[i,0] / 100\n",
    "\n",
    "logreg = LogisticRegression(penalty='l1', C = 1/10)\n",
    "logreg.fit(X_new, y_rest)\n",
    "y_pred = logreg.predict(X_new)  \n",
    "tr_error = np.mean(y_pred != y_rest)\n",
    "\n",
    "y_pred = logreg.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "    \n",
    "print(\"after scale training error %.3f\" % tr_error)\n",
    "print(\"after scale test error %.3f\" % te_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7\n",
    "## Here we are fitting our neural net.\n",
    "i = 0\n",
    "alpha = [0.00005, 0.0001, 0.00015, 0.0002, 0.00025, 0.0003, 0.00035, 0.0004]\n",
    "mlp_tr_err = np.zeros((n_splits,8))\n",
    "mlp_va_err = np.zeros((n_splits,8))\n",
    "for train, valid in kf.split(X_rest):\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = X[train], X[valid], y[train], y[valid]\n",
    "    y_train = y_train.ravel()\n",
    "    y_valid = y_valid.ravel()\n",
    "    \n",
    "    index = 0\n",
    "    for a in alpha:\n",
    "\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(100, 1000, 100), alpha=a)\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = mlp.predict(X_train)  \n",
    "        tr_error = np.mean(y_pred != y_train)\n",
    "\n",
    "        y_pred = mlp.predict(X_valid)  \n",
    "        va_error = np.mean(y_pred != y_valid)\n",
    "\n",
    "        mlp_tr_err[i,index] = tr_error    \n",
    "        mlp_va_err[i,index] = va_error\n",
    "        index = index + 1\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "print(\"Min training error %.3f\" % np.min(np.mean(mlp_tr_err, axis=0)))\n",
    "print(\"Min validation error %.3f\" % np.min(np.mean(mlp_va_err, axis=0)))\n",
    "\n",
    "print(\"Best alpha during training: {0}\".format(alpha[np.argmin(np.mean(mlp_tr_err, axis=0))]))\n",
    "print(\"Best alpha during validation: {0}\".format(alpha[np.argmin(np.mean(mlp_va_err, axis=0))]))\n",
    "\n",
    "mlp = MLPClassifier(alpha=alpha[np.argmin(np.mean(mlp_va_err, axis=0))])\n",
    "mlp.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = mlp.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "print(\"MLP test error: %.3f\" % te_error)\n",
    "\n",
    "## Here is an example of the results returned:\n",
    "## Min training error 0.266\n",
    "## Min validation error 0.271\n",
    "## Best alpha during training: 0.0004\n",
    "## Best alpha during validation: 0.0004\n",
    "## MLP test error: 0.239\n",
    "\n",
    "## These results are much better than our dummy classifier, but worse than logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are fitting a decision tree. For each fold, we calculate the training and validation error\n",
    "## over 15 different depths and store them in a 10x15 matrix.\n",
    "i = 0\n",
    "depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "tree_tr_err = np.zeros((n_splits,15))\n",
    "tree_va_err = np.zeros((n_splits,15))\n",
    "for train, valid in kf.split(X_rest):\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = X[train], X[valid], y[train], y[valid]\n",
    "    y_train = y_train.ravel()\n",
    "    y_valid = y_valid.ravel()\n",
    "    \n",
    "    index = 0\n",
    "    for j in depths:\n",
    "        tree = DecisionTreeClassifier(max_depth=j)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_train)  \n",
    "        tr_error = np.mean(y_pred != y_train)\n",
    "\n",
    "        y_pred = tree.predict(X_valid)  \n",
    "        va_error = np.mean(y_pred != y_valid)\n",
    "\n",
    "        tree_tr_err[i,index] = tr_error    \n",
    "        tree_va_err[i,index] = va_error\n",
    "        index = index + 1\n",
    "        \n",
    "## We report the minimum mean of the training and validation error over our 10 folds\n",
    "print(\"Min training error: %.3f\" % np.min(np.mean(tree_tr_err, axis=0)))\n",
    "print(\"Min validation error: %.3f\" % np.min(np.mean(tree_va_err, axis=0)))\n",
    "\n",
    "## Output the depth that corresponds to the minimum training and validation error\n",
    "print(\"Best depth during training: {0}\".format(depths[np.argmin(np.mean(tree_tr_err, axis=0))]))\n",
    "print(\"Best depth during validation: {0}\".format(depths[np.argmin(np.mean(tree_va_err, axis=0))]))\n",
    "\n",
    "## Here we fit the model to the training data using the best depth that was found during validation.\n",
    "## We then use this model to calculate the test error  \n",
    "tree = DecisionTreeClassifier(max_depth=depths[np.argmin(np.mean(tree_va_err, axis=0))])\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "print(\"tree test error: %.3f\" % te_error)\n",
    "\n",
    "## Here is an example of the results returned:\n",
    "## Min training error: 0.010\n",
    "## Min validation error: 0.016\n",
    "## Best depth during training: 15\n",
    "## Best depth during validation: 5\n",
    "## tree test error: 0.178\n",
    "\n",
    "## These results are much better than our dummy classifier, and actually slightly better than logistic\n",
    "## regression (validation error: 0.194 vs. 0.016, test error: 0.187 vs. 178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are useing stochastic gradient descent. For each fold, we calculate the training and validation error\n",
    "## over 7 different values of regularization and store them in a 10x7 matrix. We are again using L1 loss.\n",
    "i = 0\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "sgd_tr_err = np.zeros((n_splits,len(alpha)))\n",
    "sgd_va_err = np.zeros((n_splits,len(alpha)))\n",
    "for train, valid in kf.split(X_rest):\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = X[train], X[valid], y[train], y[valid]\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    y_valid = y_valid.ravel()\n",
    "    \n",
    "    if(i==0):\n",
    "        log_X_train = X_train\n",
    "        log_X_valid = X_valid\n",
    "    \n",
    "    index = 0\n",
    "    for a in alpha:\n",
    "        sgd = SGDClassifier(penalty='l1', alpha=a, max_iter=200, n_jobs=-1, random_state=40)\n",
    "        sgd.fit(X_train, y_train)\n",
    "        y_pred = sgd.predict(X_train)  \n",
    "        tr_error = np.mean(y_pred != y_train)\n",
    "\n",
    "        y_pred = sgd.predict(X_valid)  \n",
    "        va_error = np.mean(y_pred != y_valid)\n",
    "\n",
    "        sgd_tr_err[i,index] = tr_error    \n",
    "        sgd_va_err[i,index] = va_error\n",
    "        index = index + 1\n",
    "    i = i + 1\n",
    "\n",
    "## We report the minimum mean of the training and validation error over our 10 folds\n",
    "print(\"Min training error %.3f\" % np.min(np.mean(sgd_tr_err, axis=0)))\n",
    "print(\"Min validation error %.3f\" % np.min(np.mean(sgd_va_err, axis=0)))\n",
    "\n",
    "## Output the regularization strength that corresponds to the minimum training and validation error\n",
    "print(\"Best alpha during training: {0}\".format(alpha[np.argmin(np.mean(sgd_tr_err, axis=0))]))\n",
    "print(\"Best alpha during validation: {0}\".format(alpha[np.argmin(np.mean(sgd_va_err, axis=0))]))\n",
    "\n",
    "## Here we fit the model to the training data using the best regularization strength that was found \n",
    "## during validation. We then use this model to calculate the test error\n",
    "sgd = SGDClassifier(penalty='l1', alpha=alpha[np.argmin(np.mean(sgd_va_err, axis=0))], max_iter=200, n_jobs=-1, random_state=40)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "print(\"sgd test error: %.3f\" % te_error)\n",
    "\n",
    "## Here is an example of the results returned: (note, the results vary to due the randomness of sgd)\n",
    "## Min training error 0.226\n",
    "## Min validation error 0.226\n",
    "## Best alpha during training: 100\n",
    "## Best alpha during validation: 100\n",
    "## sgd test error: 0.229\n",
    "\n",
    "## The results are still much better than our dummy classifier, but not as good as our logistic regression\n",
    "## or decision tree. Interestingly, our test error is lower than our classification and validation error which\n",
    "## again may be due to the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. (Note: This takes a long time to run)\n",
    "## Here we are optimizing the alpha values and hidden_layer_sizes of our nerual net.\n",
    "## This is the parameter grid to be used \n",
    "param_grid = [\n",
    "  {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "   'hidden_layer_sizes': [(50,), (100,), (150), (200,), (250,), (300,)],\n",
    "   'random_state': [42]}\n",
    " ]\n",
    "\n",
    "y_rest = y_rest.ravel()\n",
    "\n",
    "## Calculate the best parameters for the model\n",
    "model = MLPClassifier()\n",
    "mlp = GridSearchCV(model, param_grid)\n",
    "mlp.fit(X_rest, y_rest)\n",
    "\n",
    "## Using the best parameters, calculate the test error\n",
    "y_pred = mlp.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "\n",
    "## Output the test error and corresponding parameters\n",
    "print(\"mlp test error: %.3f\" % te_error)\n",
    "print(mlp.best_params_)\n",
    "\n",
    "## Here is an example of the results: \n",
    "## mlp test error: 0.222\n",
    "## {'alpha': 1000, 'hidden_layer_sizes': (250,), 'random_state': 42}\n",
    "\n",
    "## As we can see, our test error has decreased from 0.239 to 0.222. This is quite a lackluster gain, which\n",
    "## suggests that we have optimized the wrong features, or that neural networks don't perform as well on this\n",
    "## data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9\n",
    "## Since we don't have many features, feature selection won't be very benificial\n",
    "y = y.ravel()\n",
    "features = [22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10]\n",
    "for f in features:\n",
    "    \n",
    "    ## Print the argmin and argmax of our feature selection\n",
    "    selector = SelectKBest(f_regression, k=f)\n",
    "    selector.fit(X, y)\n",
    "    print(\"min {0}\".format(np.argmin(selector.scores_)))\n",
    "    print(\"max {0}\".format(np.argmax(selector.scores_)))\n",
    "\n",
    "## The argmin consistently = 16 and the argmax consistently = 5.\n",
    "## This tells us that bill_amt_6 is not very relevant and that pay_0 is the most\n",
    "## relevant feature.\n",
    "\n",
    "X_new = X_rest\n",
    "y_rest = y_rest.ravel()\n",
    "\n",
    "logreg = LogisticRegression(penalty='l1', C = 1/10)\n",
    "logreg.fit(X_new, y_rest)\n",
    "y_pred = logreg.predict(X_new)  \n",
    "tr_error = np.mean(y_pred != y_rest)\n",
    "\n",
    "y_pred = logreg.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "    \n",
    "print(\"before scale training error %.3f\" % tr_error)\n",
    "print(\"before scale test error %.3f\" % te_error)\n",
    "\n",
    "for i in range(len(y_rest)):\n",
    "    X_new[i,16] = X_new[i,16] / 100\n",
    "    X_new[i,5] = X_new[i,5] * 100\n",
    "\n",
    "logreg = LogisticRegression(penalty='l1', C = 1/10)\n",
    "logreg.fit(X_new, y_rest)\n",
    "y_pred = logreg.predict(X_new)  \n",
    "tr_error = np.mean(y_pred != y_rest)\n",
    "\n",
    "y_pred = logreg.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "\n",
    "\n",
    "print(\"after scale training error %.3f\" % tr_error)\n",
    "print(\"after scale test error %.3f\" % te_error)\n",
    "\n",
    "## From the results of running logistic regression before and after\n",
    "## weighing bill_amt_6 and pay_0, we can see that feature selection makes\n",
    "## very little difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.\n",
    "## My best classifier was a decision tree of depth 5\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_rest, y_rest)\n",
    "y_pred = tree.predict(X_rest) \n",
    "tr_error = np.mean(y_pred != y_rest)\n",
    "\n",
    "y_pred = tree.predict(X_test)  \n",
    "te_error = np.mean(y_pred != y_test)\n",
    "\n",
    "print(\"tree training error: %.3f\" % tr_error)\n",
    "print(\"final test error: %.3f\" % te_error)\n",
    "\n",
    "## Output example:\n",
    "## tree training error: 0.176\n",
    "## tree test error: 0.221\n",
    "\n",
    "## Here our training error and test error vary slighty. Our tree is overfitting which results in \n",
    "## the difference between training and test error. Since we used the validation error to chose the depth\n",
    "## of the decision tree, this has lead to optimization bias. Our minimum validation error from before is\n",
    "## 0.016, which really demonstrates that we have an optimiation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Very short answer questions\n",
    "rubric={reasoning:7}\n",
    "\n",
    "1. Why is it difficult for a standard collaborative filtering model to make good predictions for new items?\n",
    "2. Consider a fully connected neural network with layer sizes (10,20,20,5); that is, the input dimensionality is 10, there are two hidden layers each of size 20, and the output dimensionality is 5. How many parameters does the network have, including biases?\n",
    "3. Why do we need nonlinear activation functions in neural networks?\n",
    "4. Assuming we could globally minimize the neural network objectve, how does the depth of a neural network affect the fundamental trade-off?\n",
    "5. List 3 forms of regularization we use to prevent overfitting in neural networks.\n",
    "6. Assuming we could globally minimize the neural network objectve, how would the size of the filters in a convolutational neural network affect the fundamental trade-off?\n",
    "7. Why do people say convolutional neural networks just a special case of a fully-connected (regular) neural networks? What does this imply about the number of learned parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. People tend to be excited about new things, so collaboartive filtering doesn't do a good job of predicting new    items\n",
    "2. (10x20) + (20x20) + (20x5) = 700 weights. 20 + 20 + 5 = 45 biases. 700 + 45 = 745 parameters\n",
    "3. If we do not user a non-linear activation function, then our output can be reproduced as a linear combination\n",
    "   of the inputs, effectively causing our neural network to act as though it is one layer deep, regardless of its\n",
    "   complexity\n",
    "4. As depth increases, training error decreases and the model starts to overfit. Regularization can help to \n",
    "   minimize overfitting as depth increases\n",
    "5. L2 regularization, early stopping, and dropout\n",
    "6. Increasing the size of the filter would decrease the training error, but cause the model to overfit\n",
    "7. In a convolutional neural network, each neuron isn't connected to every neuron in the previous layer like\n",
    "   a fully-connected neural network. This means that a convolutional neural network has less parameters than a \n",
    "   fully-connected neural network"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
